{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "Iiuc9BYNVPyI"
      },
      "outputs": [],
      "source": [
        "# !cp /content/drive/MyDrive/vandecia/denseGroupTPE/database.zip ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpwCNPa7W2-R",
        "outputId": "6d01bcb7-6b2f-42f2-e60c-9d9825786365"
      },
      "outputs": [],
      "source": [
        "# !unzip database.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sPMrBQw2XLgY",
        "outputId": "a6e1bd82-f130-49a2-a029-172afa3ecc47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: hyperopt in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from hyperopt) (1.26.4)\n",
            "Requirement already satisfied: scipy in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from hyperopt) (1.15.1)\n",
            "Requirement already satisfied: six in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from hyperopt) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from hyperopt) (3.4.2)\n",
            "Requirement already satisfied: future in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from hyperopt) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from hyperopt) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from hyperopt) (3.1.1)\n",
            "Requirement already satisfied: py4j in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from hyperopt) (0.10.9.9)\n",
            "Requirement already satisfied: pymongo in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (4.11.1)\n",
            "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from pymongo) (2.7.0)\n",
            "Requirement already satisfied: nbconvert in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (7.16.6)\n",
            "Requirement already satisfied: beautifulsoup4 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (4.13.3)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert) (6.2.0)\n",
            "Requirement already satisfied: defusedxml in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (0.7.1)\n",
            "Requirement already satisfied: jinja2>=3.0 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (3.1.5)\n",
            "Requirement already satisfied: jupyter-core>=4.7 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (5.7.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (0.3.0)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (3.0.2)\n",
            "Requirement already satisfied: mistune<4,>=2.0.3 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (3.1.2)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (0.10.2)\n",
            "Requirement already satisfied: nbformat>=5.7 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (5.10.4)\n",
            "Requirement already satisfied: packaging in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (24.2)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (1.5.1)\n",
            "Requirement already satisfied: pygments>=2.4.1 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (2.19.1)\n",
            "Requirement already satisfied: traitlets>=5.1 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbconvert) (5.14.3)\n",
            "Requirement already satisfied: webencodings in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert) (0.5.1)\n",
            "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from bleach[css]!=5.0.0->nbconvert) (1.4.0)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from jupyter-core>=4.7->nbconvert) (4.3.6)\n",
            "Requirement already satisfied: typing-extensions in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from mistune<4,>=2.0.3->nbconvert) (4.12.2)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbclient>=0.5.0->nbconvert) (8.6.3)\n",
            "Requirement already satisfied: fastjsonschema>=2.15 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert) (2.21.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from nbformat>=5.7->nbconvert) (4.23.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from beautifulsoup4->nbconvert) (2.6)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert) (0.23.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (2.9.0.post0)\n",
            "Requirement already satisfied: pyzmq>=23.0 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (26.2.1)\n",
            "Requirement already satisfied: tornado>=6.2 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (6.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->nbclient>=0.5.0->nbconvert) (1.17.0)\n",
            "Requirement already satisfied: pydot in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (3.0.4)\n",
            "Requirement already satisfied: graphviz in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (0.20.3)\n",
            "Requirement already satisfied: pyparsing>=3.0.9 in /home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages (from pydot) (3.2.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install hyperopt\n",
        "!pip install pymongo\n",
        "!pip install nbconvert\n",
        "!pip install pydot graphviz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "PTVDpS_EXTS-"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: não foi possível criar o diretório “results”: Arquivo existe\n"
          ]
        }
      ],
      "source": [
        "!mkdir results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "PZW8aOQrXR9s"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import sklearn as sk\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import random\n",
        "import glob\n",
        "from sklearn.metrics import accuracy_score # deu problema no ultimo treino e agora adicionei isso para que a variavel acc funcione \n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import layers\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\n",
        "from keras import layers\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn import metrics\n",
        "from keras import metrics\n",
        "from keras.models import load_model, Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "from hyperopt import hp\n",
        "from hyperopt import hp, tpe, Trials, fmin, STATUS_OK\n",
        "from hyperopt import STATUS_OK, STATUS_FAIL\n",
        "\n",
        "\n",
        "import json\n",
        "from bson import json_util\n",
        "\n",
        "import keras.backend as K\n",
        "from sklearn.utils import class_weight\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import classification_report, confusion_matrix, recall_score, precision_score, f1_score\n",
        "\n",
        "import traceback\n",
        "import pickle\n",
        "import uuid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UErBg9llXWvB",
        "outputId": "45d34789-40ce-479b-eb1b-20e233fba604"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 5224 images belonging to 3 classes.\n",
            "Found 624 images belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "#define o tamanho padrão das imagens que serão passadas na rede, sendo que a mesma aceita imagens maiores que o padrão definido da VGG16 (255x255)\n",
        "img_width = 128\n",
        "img_height = 128\n",
        "\n",
        "batch_size = 8 #batch_size para o treino\n",
        "\n",
        "#define o batch_size de validação, das imagens de acordo com a memória disponivél na máquina\n",
        "batch_size_val = 1\n",
        "\n",
        "#define as épocas\n",
        "epochs = 5\n",
        "\n",
        "# class_weight = {0: 1.48, 1: 4.14, 2:11.49}\n",
        "class_weight = {0: 3.15, 1: 3.9, 2:3.9}\n",
        "\n",
        "RESULTS_DIR = \"results/\" #pasta para salvar os resultados dos treinamentos\n",
        "\n",
        "train_data_dir = \"chest_pneumonia/train\"\n",
        "# validation_data_dir = \"database/split1/val\"\n",
        "test_data_dir = \"chest_pneumonia/test\"\n",
        "\n",
        "num_classes_exp = 3\n",
        "\n",
        "\n",
        "space = {\n",
        "    'num_blocks': hp.choice('num_blocks', [3]),\n",
        "    'num_layers_per_block' : hp.choice('num_layers_per_block', [2]),\n",
        "    'growth_rate': hp.choice('growth_rate', [32]),\n",
        "    'dropout_rate' : hp.uniform('dropout_rate', 0.2, 0.35),\n",
        "    'compress_factor' : hp.choice('compress_factor', [0.5]),\n",
        "    'num_filters' : hp.choice('num_filters', [64])\n",
        "}\n",
        "\n",
        "\n",
        "#DataGenerator utilizado para fazer o augmentation on the batch\n",
        "datagen = ImageDataGenerator(rescale=1.,\n",
        "    featurewise_center=True,\n",
        "    rotation_range=10,\n",
        "    width_shift_range=.1,\n",
        "    height_shift_range=.1,\n",
        "    shear_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False,\n",
        "    fill_mode=\"reflect\") #generator de treino\n",
        "\n",
        "validgen = ImageDataGenerator(rescale=1., featurewise_center=True) #generator de teste e validação, evita-se realizar alterações nas imagens\n",
        "\n",
        "#como as imagens apresentam um tamanho maior que o padrão, deve-se fazer uma normalização das mesmas para que sejam aceitas na rede\n",
        "datagen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n",
        "validgen.mean=np.array([103.939, 116.779, 123.68],dtype=np.float32).reshape(1,1,3)\n",
        "\n",
        "#definindo os geradores para cada pasta\n",
        "train_gen = datagen.flow_from_directory( #generator para treino\n",
        "    train_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True)\n",
        "\n",
        "# val_gen = validgen.flow_from_directory( #generator para validação\n",
        "#     validation_data_dir,\n",
        "#     target_size=(img_height, img_width),\n",
        "#     batch_size=batch_size_val,\n",
        "#     class_mode=\"categorical\",\n",
        "#     shuffle=True)\n",
        "\n",
        "test_gen = validgen.flow_from_directory( #generator para teste\n",
        "    test_data_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size_val,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False)\n",
        "\n",
        "\n",
        "#pega a quantidade de amostras de cada generator\n",
        "train_samples = len(train_gen.filenames)\n",
        "# validation_samples = len(val_gen.filenames)\n",
        "test_samples = len(test_gen.filenames)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "4A6POlHOXc5h"
      },
      "outputs": [],
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "def keras_model_memory_usage_in_bytes(model, *, batch_size: int):\n",
        "\n",
        "    \"\"\"\n",
        "    Return the estimated memory usage of a given Keras model in bytes.\n",
        "    This includes the model weights and layers, but excludes the dataset.\n",
        "\n",
        "    The model shapes are multipled by the batch size, but the weights are not.\n",
        "\n",
        "    Args:\n",
        "        model: A Keras model.\n",
        "        batch_size: The batch size you intend to run the model with. If you\n",
        "            have already specified the batch size in the model itself, then\n",
        "            pass `1` as the argument here.\n",
        "    Returns:\n",
        "        An estimate of the Keras model's memory usage in bytes.\n",
        "\n",
        "    \"\"\"\n",
        "    default_dtype = tf.keras.backend.floatx()\n",
        "    shapes_mem_count = 0\n",
        "    internal_model_mem_count = 0\n",
        "    for layer in model.layers:\n",
        "        if isinstance(layer, tf.keras.Model):\n",
        "            internal_model_mem_count += keras_model_memory_usage_in_bytes(\n",
        "                layer, batch_size=batch_size\n",
        "            )\n",
        "        single_layer_mem = tf.as_dtype(layer.dtype or default_dtype).size\n",
        "        out_shape = layer.output\n",
        "        if isinstance(out_shape, list):\n",
        "            out_shape = out_shape[0]\n",
        "        for s in out_shape:\n",
        "            if s is None:\n",
        "                continue\n",
        "            single_layer_mem *= s\n",
        "        shapes_mem_count += single_layer_mem\n",
        "\n",
        "    trainable_count = sum(\n",
        "        [tf.keras.backend.count_params(p) for p in model.trainable_weights]\n",
        "    )\n",
        "    non_trainable_count = sum(\n",
        "        [tf.keras.backend.count_params(p) for p in model.non_trainable_weights]\n",
        "    )\n",
        "\n",
        "    total_memory = (\n",
        "        batch_size * shapes_mem_count\n",
        "        + internal_model_mem_count\n",
        "        + trainable_count\n",
        "        + non_trainable_count\n",
        "    )\n",
        "    return total_memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "OeuX5d7RXeQd"
      },
      "outputs": [],
      "source": [
        "def save_json_result(model_name, result):\n",
        "    \"\"\"Save json to a directory and a filename.\"\"\"\n",
        "    result_name = '{}.txt.json'.format(model_name)\n",
        "    if not os.path.exists(RESULTS_DIR):\n",
        "        os.makedirs(RESULTS_DIR)\n",
        "    with open(os.path.join(RESULTS_DIR, result_name), 'w') as f:\n",
        "        json.dump(\n",
        "            result, f,\n",
        "            default=json_util.default, sort_keys=True,\n",
        "            indent=4, separators=(',', ': ')\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKX77fEwXvnH"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "N63ALJ-dZScL"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau, EarlyStopping\n",
        "from keras import layers\n",
        "from keras.optimizers import SGD, Adam\n",
        "from sklearn import metrics\n",
        "from keras import metrics\n",
        "\n",
        "eps = 1.1e-5\n",
        "'''\n",
        "def H( inputs, num_filters , dropout_rate ):\n",
        "    x = layers.BatchNormalization( epsilon=eps )( inputs )\n",
        "    x = layers.Activation('relu')(x)\n",
        "    x = layers.ZeroPadding2D((1, 1))(x)\n",
        "    x = layers.SeparableConv2D(num_filters, kernel_size=(3, 3), use_bias=False , kernel_initializer='he_normal' )(x)\n",
        "    x = layers.Dropout(rate=dropout_rate )(x)\n",
        "    return x\n",
        "'''\n",
        "def H( inputs, num_filters , dropout_rate ):\n",
        "    x = layers.BatchNormalization( epsilon=eps )( inputs )\n",
        "    x = layers.Activation('relu')(x)\n",
        "\n",
        "    out_conv = []\n",
        "    for i in [(1,1),(3,3),(5,5),(0,0)]:\n",
        "        p = x\n",
        "        if i == (1,1):\n",
        "                p = layers.Conv2D(num_filters, (1,1), padding=\"same\",activation=\"relu\")(p)\n",
        "                out_conv.append(layers.Conv2D(num_filters, (1,1), padding=\"same\",activation=\"relu\")(p))\n",
        "        elif i == (0,0):\n",
        "                p = layers.MaxPool2D(pool_size=(2, 2), padding=\"same\",strides=(1,1))(p)\n",
        "                out_conv.append(layers.Conv2D(num_filters, (1,1), padding=\"same\",activation=\"relu\")(p))\n",
        "        else:\n",
        "                p = layers.Conv2D(num_filters, (1,1), padding=\"same\",activation=\"relu\")(p)\n",
        "                p = layers.SeparableConv2D(num_filters, i, padding=\"same\",activation=\"relu\")(p)\n",
        "                out_conv.append(layers.SeparableConv2D(num_filters, i, padding=\"same\",activation=\"relu\")(p))\n",
        "\n",
        "    x = layers.concatenate(out_conv, axis = -1)\n",
        "    x = layers.Dropout(rate=dropout_rate )(x)\n",
        "    return x\n",
        "\n",
        "def transition(inputs, num_filters , compression_factor , dropout_rate ):\n",
        "    # compression_factor is the 'θ'\n",
        "    x = layers.BatchNormalization( epsilon=eps )(inputs)\n",
        "    x = layers.Activation('relu')(x)\n",
        "    num_feature_maps = inputs.shape[1] # The value of 'm'\n",
        "\n",
        "    x = layers.Conv2D(int(np.floor(num_feature_maps * compression_factor)) ,\n",
        "                               kernel_size=(1, 1), use_bias=False, padding='same' , kernel_initializer='he_normal')(x)\n",
        "    x = layers.Dropout(rate=dropout_rate)(x)\n",
        "\n",
        "    x = layers.AveragePooling2D(pool_size=(2, 2))(x)\n",
        "    return x\n",
        "\n",
        "def dense_block( inputs, num_layers, num_filters, growth_rate , dropout_rate ):\n",
        "    for i in range(num_layers): # num_layers is the value of 'l'\n",
        "        conv_outputs = H(inputs, num_filters , dropout_rate )\n",
        "        inputs = layers.Concatenate()([conv_outputs, inputs])\n",
        "        num_filters += growth_rate # To increase the number of filters for each layer.\n",
        "    return inputs, num_filters\n",
        "\n",
        "def get_model(input_shape,\n",
        "           num_blocks,\n",
        "           num_layers_per_block,\n",
        "           growth_rate,\n",
        "           dropout_rate,\n",
        "           compress_factor,\n",
        "           num_filters,\n",
        "           num_classes):\n",
        "    '''\n",
        "    input_shape = ( 32 , 32 , 3 )\n",
        "    num_blocks = 3\n",
        "    num_layers_per_block = 4\n",
        "    growth_rate = 16\n",
        "    dropout_rate = 0.4\n",
        "    compress_factor = 0.5\n",
        "\n",
        "    num_filters = 16\n",
        "    '''\n",
        "\n",
        "    inputs = layers.Input( shape=input_shape )\n",
        "    x = layers.Conv2D( num_filters , kernel_size=( 3 , 3 ) , padding=\"same\", use_bias=False, kernel_initializer='he_normal')( inputs )\n",
        "    for i in range( num_blocks ):\n",
        "        x, num_filters = dense_block( x, num_layers_per_block , num_filters, growth_rate , dropout_rate )\n",
        "        x = transition(x, num_filters , compress_factor , dropout_rate )\n",
        "\n",
        "    x = layers.GlobalAveragePooling2D()( x )\n",
        "    x = layers.Dense(256, activation='relu')(x)\n",
        "    x = layers.Dense( num_classes )( x )\n",
        "    outputs = layers.Activation( 'softmax' )( x )\n",
        "\n",
        "    model = Model( inputs , outputs )\n",
        "    model.compile( loss='categorical_crossentropy' ,optimizer=Adam(),\n",
        "                    metrics=[ 'accuracy',\n",
        "                              metrics.Recall(thresholds=0.5, class_id=0,name='r_bacteria'),\n",
        "                              metrics.Recall(thresholds=0.5, class_id=1,name='r_normal'),\n",
        "                              metrics.Recall(thresholds=0.5, class_id=2,name='r_viral')])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "dCQhxSxBagaN"
      },
      "outputs": [],
      "source": [
        "def build_and_train(hype_space):\n",
        "    print (hype_space)\n",
        "\n",
        "    model_final = get_model(input_shape=(img_width, img_height, 3),\n",
        "            num_blocks = int(hype_space['num_blocks']),\n",
        "            num_layers_per_block = int(hype_space['num_layers_per_block']),\n",
        "            growth_rate = int(hype_space['growth_rate']),\n",
        "            dropout_rate = hype_space['dropout_rate'],\n",
        "            compress_factor = hype_space['compress_factor'],\n",
        "            num_filters = hype_space['num_filters'],\n",
        "            num_classes = num_classes_exp)\n",
        "\n",
        "    #model_size = keras_model_memory_usage_in_bytes(model = model_final,\n",
        "    #                    batch_size = batch_size)\n",
        "    #model_size = model_size/1000000000\n",
        "\n",
        "    ##print(\"Model size: \" + str(model_size) )\n",
        "    # if (model_size > 10.5):\n",
        "    #     model_name = \"model_\" + str(uuid.uuid4())[:5]\n",
        "    #     result = {\n",
        "    #         'space': hype_space,\n",
        "    #         'status': STATUS_FAIL\n",
        "    #     }\n",
        "    #     return model_final, model_name, result\n",
        "\n",
        "    #model_final = load_model('weights_best_etapa1.hdf5')\n",
        "\n",
        "    #inicio da fase de treino\n",
        "    #as imagens são passadas na rede\n",
        "    early_stopping = EarlyStopping(monitor='loss', patience=4,\n",
        "                                    verbose=1, mode='auto')\n",
        "    checkpoint = ModelCheckpoint('weights_best_etapa1.keras', monitor='loss',\n",
        "                                verbose=1,\n",
        "                                save_best_only=True, mode='auto')\n",
        "\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=3, verbose=1)\n",
        "\n",
        "    model_final.fit(train_gen,\n",
        "                                epochs=epochs,\n",
        "                                steps_per_epoch=int(train_samples/batch_size),\n",
        "                                validation_data=test_gen,\n",
        "                                validation_steps=batch_size_val,\n",
        "                                class_weight = class_weight,\n",
        "                                verbose=1, callbacks=[early_stopping,checkpoint,reduce_lr])\n",
        "\n",
        "    preds = model_final.predict(test_gen, test_samples) #realiza o teste de classificação das imagens na rede\n",
        "    y_pred = np.argmax(preds, axis=1)\n",
        "    #print(classification_report(test_gen.classes, y_pred))#, target_names=target_names))\n",
        "    acc = accuracy_score(test_gen.classes, y_pred) #calcula o acurácia era metrics.accuracy_score....\n",
        "    class_report = classification_report(test_gen.classes, y_pred, output_dict=True)#, target_names=target_names)\n",
        "\n",
        "    # model_pesos = load_model('weights_best_etapa1.hdf5')\n",
        "    # preds = model_pesos.predict(test_gen, test_samples) #realiza o teste de classificação das imagens na rede\n",
        "    # y_pred = np.argmax(preds, axis=1)\n",
        "    # acc_p_1 = accuracy_score(test_gen.classes, y_pred) #calcula o acurácia\n",
        "    # class_report_p_1 = classification_report(test_gen.classes, y_pred, output_dict=True)#, target_names=target_names)\n",
        "\n",
        "    # del model_pesos\n",
        "\n",
        "    model_name = \"model_{}_{}\".format(str(acc), str(uuid.uuid4())[:5])\n",
        "    plot_model(model_final, to_file= RESULTS_DIR + model_name + '_plot.png', show_shapes=True, show_layer_names=True)\n",
        "\n",
        "    result = {\n",
        "        'loss': 1-acc,\n",
        "        'acurracy': acc,\n",
        "        'report': class_report,\n",
        "        # 'acurracy_p_1': acc_p_1,\n",
        "        # 'report_p_1': class_report_p_1,\n",
        "        'model_name': model_name,\n",
        "        'space': hype_space,\n",
        "        'status': STATUS_OK\n",
        "    }\n",
        "\n",
        "    print(result)\n",
        "\n",
        "    return model_final, model_name, result\n",
        "\n",
        "def optimize_cnn(hype_space):\n",
        "    \"\"\"Build a convolutional neural network and train it.\"\"\"\n",
        "    try:\n",
        "        model, model_name, result = build_and_train(hype_space)\n",
        "\n",
        "        # Save training results to disks with unique filenames\n",
        "        save_json_result(model_name, result)\n",
        "\n",
        "        K.clear_session()\n",
        "        del model\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as err:\n",
        "        try:\n",
        "            K.clear_session()\n",
        "        except:\n",
        "            pass\n",
        "        err_str = str(err)\n",
        "        print(err_str)\n",
        "        traceback_str = str(traceback.format_exc())\n",
        "        print(traceback_str)\n",
        "        return {\n",
        "            'status': STATUS_FAIL,\n",
        "            'err': err_str,\n",
        "            'traceback': traceback_str\n",
        "        }\n",
        "\n",
        "    print(\"\\n\\n\")\n",
        "\n",
        "def run_a_trial():\n",
        "    \"\"\"Run one TPE meta optimisation step and save its results.\"\"\"\n",
        "    max_evals = nb_evals = 1\n",
        "\n",
        "    print(\"Attempt to resume a past training if it exists:\")\n",
        "\n",
        "    try:\n",
        "        # https://github.com/hyperopt/hyperopt/issues/267\n",
        "        trials = pickle.load(open(\"results.pkl\", \"rb\"))\n",
        "        print(\"Found saved Trials! Loadin..\")\n",
        "        max_evals = len(trials.trials) + nb_evals\n",
        "        print(\"Rerunning from {} trials to add another one.\".format(\n",
        "            len(trials.trials)))\n",
        "    except:\n",
        "        trials = Trials()\n",
        "        print(\"Starting from scratch: new trials.\")\n",
        "\n",
        "    best = fmin(\n",
        "        optimize_cnn,\n",
        "        space,\n",
        "        algo=tpe.suggest,\n",
        "        trials=trials,\n",
        "        max_evals=max_evals\n",
        "    )\n",
        "    pickle.dump(trials, open(\"results.pkl\", \"wb\"))\n",
        "\n",
        "    print(\"\\nOPTIMIZATION STEP COMPLETE.\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VeGZWLo0ajwy",
        "outputId": "a38cb3cd-fc80-4736-bdfa-8e60d3365552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempt to resume a past training if it exists:\n",
            "Starting from scratch: new trials.\n",
            "{'compress_factor': 0.5, 'dropout_rate': 0.24509868367389118, 'growth_rate': 32, 'num_blocks': 3, 'num_filters': 64, 'num_layers_per_block': 2}\n",
            "Epoch 1/5                                            \n",
            "\n",
            "  0%|          | 0/1 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-03-28 09:50:27.020938: W tensorflow/core/framework/op_kernel.cc:1768] INVALID_ARGUMENT: ValueError: Could not find callback with key=pyfunc_14 in the registry.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 259, in __call__\n",
            "    raise ValueError(f\"Could not find callback with key={token} in the \"\n",
            "\n",
            "ValueError: Could not find callback with key=pyfunc_14 in the registry.\n",
            "\n",
            "\n",
            "2025-03-28 09:50:27.020999: W tensorflow/core/kernels/data/generator_dataset_op.cc:108] Error occurred when finalizing GeneratorDataset iterator: INVALID_ARGUMENT: ValueError: Could not find callback with key=pyfunc_14 in the registry.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 259, in __call__\n",
            "    raise ValueError(f\"Could not find callback with key={token} in the \"\n",
            "\n",
            "ValueError: Could not find callback with key=pyfunc_14 in the registry.\n",
            "\n",
            "\n",
            "\t [[{{node PyFunc}}]]\n",
            "2025-03-28 09:50:27.029184: W tensorflow/core/framework/op_kernel.cc:1768] INVALID_ARGUMENT: ValueError: Could not find callback with key=pyfunc_17 in the registry.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 259, in __call__\n",
            "    raise ValueError(f\"Could not find callback with key={token} in the \"\n",
            "\n",
            "ValueError: Could not find callback with key=pyfunc_17 in the registry.\n",
            "\n",
            "\n",
            "2025-03-28 09:50:27.029232: W tensorflow/core/kernels/data/generator_dataset_op.cc:108] Error occurred when finalizing GeneratorDataset iterator: INVALID_ARGUMENT: ValueError: Could not find callback with key=pyfunc_17 in the registry.\n",
            "Traceback (most recent call last):\n",
            "\n",
            "  File \"/home/bruna/miniconda3/envs/bruna/lib/python3.10/site-packages/tensorflow/python/ops/script_ops.py\", line 259, in __call__\n",
            "    raise ValueError(f\"Could not find callback with key={token} in the \"\n",
            "\n",
            "ValueError: Could not find callback with key=pyfunc_17 in the registry.\n",
            "\n",
            "\n",
            "\t [[{{node PyFunc}}]]\n"
          ]
        }
      ],
      "source": [
        "run_a_trial()\n",
        "\n",
        "# if __name__ == \"__main__\":\n",
        "#     while True:\n",
        "\n",
        "#         # Optimize a new model with the TPE Algorithm:\n",
        "#         print(\"OPTIMIZING NEW MODEL:\")\n",
        "#         try:\n",
        "#             run_a_trial()\n",
        "#         except Exception as err:\n",
        "#             err_str = str(err)\n",
        "#             print(err_str)\n",
        "#             #traceback_str = str(traceback.format_exc())\n",
        "#             #print(traceback_str)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "bruna",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
